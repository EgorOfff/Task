{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b862b29665bb48439cc064f3b1de2514": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_28e2eae3e5f749309a9c3c9f6f9c5afc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1a5e41e8fc5c47a5b6793b8b56cffc40",
              "IPY_MODEL_f18d77fc078847439a12838ae9b4f49e"
            ]
          }
        },
        "28e2eae3e5f749309a9c3c9f6f9c5afc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1a5e41e8fc5c47a5b6793b8b56cffc40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b750a143aa014b2eb86606529ca80b8b",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 49388949,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 49388949,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_01b6832ac7d44f73bc9b5e61a68cf1a4"
          }
        },
        "f18d77fc078847439a12838ae9b4f49e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_750b8eae37cd44d1aa84d29492d3cb7e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 47.1M/47.1M [00:01&lt;00:00, 25.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eea589e9125141c1994d620cb69ade27"
          }
        },
        "b750a143aa014b2eb86606529ca80b8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "01b6832ac7d44f73bc9b5e61a68cf1a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "750b8eae37cd44d1aa84d29492d3cb7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eea589e9125141c1994d620cb69ade27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZj6SaqXPBBv",
        "outputId": "757c96d5-273d-4887-a6c1-c81297d5f404"
      },
      "source": [
        "!pip install albumentations\n",
        "!pip install efficientnet_pytorch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (0.1.12)\n",
            "Collecting imgaug<0.2.7,>=0.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/2e/748dbb7bb52ec8667098bae9b585f448569ae520031932687761165419a2/imgaug-0.2.6.tar.gz (631kB)\n",
            "\u001b[K     |████████████████████████████████| 634kB 11.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from albumentations) (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.19.5)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (0.16.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (1.15.0)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (3.2.2)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (7.1.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.1.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.5.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.8.1)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (4.4.2)\n",
            "Building wheels for collected packages: imgaug\n",
            "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imgaug: filename=imgaug-0.2.6-cp37-none-any.whl size=654019 sha256=386ca7f897a765b099813623e6ed23793b2c87ff5ee338fe95563abfdf435299\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/ec/48/0d25896c417b715af6236dbcef8f0bed136a1a5e52972fc6d0\n",
            "Successfully built imgaug\n",
            "Installing collected packages: imgaug\n",
            "  Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "Successfully installed imgaug-0.2.6\n",
            "Collecting efficientnet_pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/2e/a0/dd40b50aebf0028054b6b35062948da01123d7be38d08b6b1e5435df6363/efficientnet_pytorch-0.7.1.tar.gz\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet_pytorch) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (3.7.4.3)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-cp37-none-any.whl size=16443 sha256=c45f2c329c5cce0aff138b5a8d3cab92e79f9c0e86feec3d450843dcce12cfab\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/27/aa/c46d23c4e8cc72d41283862b1437e0b3ad318417e8ed7d5921\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: efficientnet-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imVBat1fPFzO"
      },
      "source": [
        "#импортируем все что пригодится\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import shutil\n",
        "import os\n",
        "import zipfile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "import tqdm\n",
        "import time\n",
        "import random\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import model_selection, metrics\n",
        "\n",
        "import albumentations\n",
        "from albumentations import pytorch as AT\n",
        "\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJPFgyQgPHUA"
      },
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    \n",
        "seed_everything(42)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QRAd7lCPIdU",
        "outputId": "fca0d045-69e9-4d9f-8a7a-dd7da552967a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6DQoDBWPJV6"
      },
      "source": [
        "with zipfile.ZipFile('/content/drive/MyDrive/taskonML/train.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall('/content')\n",
        "\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/taskonML/test.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall('/content')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPHSwO23PKZ3"
      },
      "source": [
        "train_dir = '/content'\n",
        "test_dir = '/content/test'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhihsMCDPMC8"
      },
      "source": [
        "train_data = pd.read_csv('/content/drive/MyDrive/taskonML/train_labels.csv')\n",
        "kl = np.asarray(train_data['sports'].unique())"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4D3Cr-6PNIo"
      },
      "source": [
        "encoder = LabelEncoder()\n",
        "train_data['num_labels'] = encoder.fit_transform(train_data['sports'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "IMyJDrVhPOTT",
        "outputId": "982af0f1-edd8-4877-f40d-068aeb03e6a9"
      },
      "source": [
        "train_data"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>sports</th>\n",
              "      <th>num_labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>./train/0.jpg</td>\n",
              "      <td>baseball</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>./train/1.jpg</td>\n",
              "      <td>formula1</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>./train/2.jpg</td>\n",
              "      <td>fencing</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>./train/3.jpg</td>\n",
              "      <td>motogp</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>./train/4.jpg</td>\n",
              "      <td>ice_hockey</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11035</th>\n",
              "      <td>./train/11035.jpg</td>\n",
              "      <td>motogp</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11036</th>\n",
              "      <td>./train/11036.jpg</td>\n",
              "      <td>motogp</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11037</th>\n",
              "      <td>./train/11037.jpg</td>\n",
              "      <td>football</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11038</th>\n",
              "      <td>./train/11038.jpg</td>\n",
              "      <td>football</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11039</th>\n",
              "      <td>./train/11039.jpg</td>\n",
              "      <td>formula1</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11040 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   image      sports  num_labels\n",
              "0          ./train/0.jpg    baseball           1\n",
              "1          ./train/1.jpg    formula1           8\n",
              "2          ./train/2.jpg     fencing           6\n",
              "3          ./train/3.jpg      motogp          13\n",
              "4          ./train/4.jpg  ice_hockey          11\n",
              "...                  ...         ...         ...\n",
              "11035  ./train/11035.jpg      motogp          13\n",
              "11036  ./train/11036.jpg      motogp          13\n",
              "11037  ./train/11037.jpg    football           7\n",
              "11038  ./train/11038.jpg    football           7\n",
              "11039  ./train/11039.jpg    formula1           8\n",
              "\n",
              "[11040 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnL0VN3fPRq_"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, file_csv, dir, transform=None, mode = 'train'):\n",
        "        self.file_csv = file_csv\n",
        "        self.dir = dir\n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.file_csv)\n",
        "    \n",
        "    #метод который позволяет нам индексировать датасет\n",
        "    def __getitem__(self, idx):\n",
        "        #считываем изображение\n",
        "        image = cv2.imread(os.path.join(self.dir, self.file_csv['image'][idx]))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        self.label = int(self.file_csv['num_labels'][idx])\n",
        "\n",
        "        \n",
        "        #применяем аугментации\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image)\n",
        "            image = augmented['image']\n",
        "        \n",
        "        if self.mode == 'train':\n",
        "            return image, float(self.label)\n",
        "        else:\n",
        "            return image, self.file_csv['image'][idx]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-T6ePXdPSNX"
      },
      "source": [
        "#зададим немного гиперпараметров\n",
        "batch_size = 32\n",
        "num_workers = 2\n",
        "img_size = 256"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMswhPdIPTTI"
      },
      "source": [
        "data_transforms = albumentations.Compose([\n",
        "    albumentations.Resize(img_size, img_size),\n",
        "    albumentations.HorizontalFlip(p=0.5),\n",
        "    albumentations.RandomBrightness(),\n",
        "    albumentations.ShiftScaleRotate(rotate_limit=15, scale_limit=0.10),\n",
        "    albumentations.HueSaturationValue(),\n",
        "    albumentations.Normalize(),\n",
        "    AT.ToTensor()\n",
        "    ])\n",
        "\n",
        "data_transforms_test = albumentations.Compose([\n",
        "    albumentations.Resize(img_size, img_size),\n",
        "    albumentations.HorizontalFlip(),\n",
        "    albumentations.RandomRotate90(),\n",
        "    albumentations.Normalize(),\n",
        "    AT.ToTensor()\n",
        "    ])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSjqDNFKPUZx"
      },
      "source": [
        "train_df, test_df = model_selection.train_test_split(\n",
        "    train_data, test_size=0.15, random_state=42, stratify=train_data.num_labels.values\n",
        ")\n",
        "train_df, valid_df = model_selection.train_test_split(\n",
        "    train_df, test_size=0.15, random_state=42, stratify=train_df.num_labels.values\n",
        ")\n",
        "\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "valid_df = valid_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4KrA7v8PX2G"
      },
      "source": [
        "train_set = CustomDataset(train_df, train_dir, transform = data_transforms)\n",
        "valid_set = CustomDataset(valid_df, train_dir, transform = data_transforms_test)\n",
        "test_set = CustomDataset(test_df, train_dir, transform = data_transforms_test)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJjI0fGyPZdb",
        "outputId": "4afb078d-c89d-4380-e61f-2a43eb52c9d4"
      },
      "source": [
        "len(train_set), len(valid_set), len(test_set)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7976, 1408, 1656)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zlcq9Rz8PaiU"
      },
      "source": [
        "trainloader = torch.utils.data.DataLoader(train_set, pin_memory=True, \n",
        "                                        batch_size=batch_size, shuffle=True, \n",
        "                                        num_workers=num_workers)\n",
        "validloader = torch.utils.data.DataLoader(valid_set, pin_memory=True, \n",
        "                                        batch_size=batch_size, shuffle=True)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(test_set, batch_size = batch_size,\n",
        "                                        num_workers = num_workers)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-sTI5ZdPdqx"
      },
      "source": [
        "#Проверим работоспособность\n",
        "samples, labels = next(iter(trainloader))\n",
        "plt.figure(figsize=(16,24))\n",
        "grid_imgs = torchvision.utils.make_grid(samples[:24])\n",
        "np_grid_imgs = grid_imgs.numpy()\n",
        "#  чтобы вывести изображение, нужно его немного преобразовать из тензора обратно в нампай\n",
        "plt.imshow(np.transpose(np_grid_imgs, (1,2,0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6sc6rKFPeID",
        "outputId": "dbc8ef62-3c58-4605-931d-3a944c9a33c1"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119,
          "referenced_widgets": [
            "b862b29665bb48439cc064f3b1de2514",
            "28e2eae3e5f749309a9c3c9f6f9c5afc",
            "1a5e41e8fc5c47a5b6793b8b56cffc40",
            "f18d77fc078847439a12838ae9b4f49e",
            "b750a143aa014b2eb86606529ca80b8b",
            "01b6832ac7d44f73bc9b5e61a68cf1a4",
            "750b8eae37cd44d1aa84d29492d3cb7e",
            "eea589e9125141c1994d620cb69ade27"
          ]
        },
        "id": "IJtOhWJlPfXg",
        "outputId": "953ca2b4-04c0-4991-d75e-fe21f93512e3"
      },
      "source": [
        "# model = EfficientNet.from_pretrained('efficientnet-b3')\n",
        "# #model"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b3-5fb5a3c3.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b3-5fb5a3c3.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b862b29665bb48439cc064f3b1de2514",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=49388949.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded pretrained weights for efficientnet-b3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC_T_75oPgxd"
      },
      "source": [
        "model._fc = nn.Linear(in_features = 1536, out_features = 22)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtiTdz7FPkCb"
      },
      "source": [
        "model = torchvision.models.resnet152(pretrained=True, progress=True)\n",
        "#заморозим веса, чтобы использовать полностью предобученную сетку\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gcnW7h4Pkmt"
      },
      "source": [
        "model.fc = nn.Linear(2048, 22)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAK885YzPozc"
      },
      "source": [
        "def train_model(model_conv, train_loader, valid_loader, criterion, optimizer, scheduler, n_epochs):\n",
        "    model_conv.to(device)\n",
        "    valid_loss_min = np.Inf\n",
        "    patience = 5\n",
        "    # сколько эпох ждем до отключения\n",
        "    p = 0\n",
        "    # иначе останавливаем обучение\n",
        "    stop = False\n",
        "\n",
        "    # количество эпох\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        print(time.ctime(), 'Epoch:', epoch)\n",
        "\n",
        "        train_loss = []\n",
        "\n",
        "        for batch_i, (data, target) in enumerate(train_loader):\n",
        "\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model_conv(data)\n",
        "            loss = criterion(output, target.long())\n",
        "            train_loss.append(loss.item())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    # запускаем валидацию\n",
        "        model_conv.eval()\n",
        "        correct = 0\n",
        "        val_loss = []\n",
        "        for batch_i, (data, target) in enumerate(valid_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model_conv(data)\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            correct += (predicted == target).sum().item()\n",
        "            loss = criterion(output, target.long())\n",
        "            val_loss.append(loss.item()) \n",
        "        \n",
        "        acc = correct / len(valid_set)\n",
        "\n",
        "        print(f'Epoch {epoch}, train loss: {np.mean(train_loss):.4f}, valid loss: {np.mean(val_loss):.4f}.')\n",
        "        print(f'Accuracy on valid set: {acc}')\n",
        "\n",
        "        valid_loss = np.mean(val_loss)\n",
        "        scheduler.step(valid_loss)\n",
        "        if valid_loss <= valid_loss_min:\n",
        "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "            valid_loss_min,\n",
        "            valid_loss))\n",
        "            torch.save(model_conv.state_dict(), 'model.pt')\n",
        "            valid_loss_min = valid_loss\n",
        "            p = 0\n",
        "\n",
        "        # проверяем как дела на валидации\n",
        "        if valid_loss > valid_loss_min:\n",
        "            p += 1\n",
        "            print(f'{p} epochs of increasing val loss')\n",
        "            if p > patience:\n",
        "                print('Stopping training')\n",
        "                stop = True\n",
        "                break        \n",
        "\n",
        "        if stop:\n",
        "            break\n",
        "    return model_conv, train_loss, val_loss"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHuTQvZlPotP"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.8, patience=2,)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTLoOykNPqVv",
        "outputId": "3579ce1c-f5fc-4598-9f30-8232edae76f6"
      },
      "source": [
        "model_resnet, train_loss, val_loss = train_model(model, trainloader, validloader, criterion, \n",
        "                              optimizer, scheduler, n_epochs=20,)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Apr 28 14:49:34 2021 Epoch: 1\n",
            "Epoch 1, train loss: 1.2515, valid loss: 0.9414.\n",
            "Accuracy on valid set: 0.7215909090909091\n",
            "Validation loss decreased (inf --> 0.941429).  Saving model ...\n",
            "Wed Apr 28 14:53:44 2021 Epoch: 2\n",
            "Epoch 2, train loss: 0.5768, valid loss: 0.7243.\n",
            "Accuracy on valid set: 0.7954545454545454\n",
            "Validation loss decreased (0.941429 --> 0.724324).  Saving model ...\n",
            "Wed Apr 28 14:57:50 2021 Epoch: 3\n",
            "Epoch 3, train loss: 0.4820, valid loss: 0.7983.\n",
            "Accuracy on valid set: 0.7642045454545454\n",
            "1 epochs of increasing val loss\n",
            "Wed Apr 28 15:01:56 2021 Epoch: 4\n",
            "Epoch 4, train loss: 0.4413, valid loss: 0.6970.\n",
            "Accuracy on valid set: 0.7954545454545454\n",
            "Validation loss decreased (0.724324 --> 0.697042).  Saving model ...\n",
            "Wed Apr 28 15:06:04 2021 Epoch: 5\n",
            "Epoch 5, train loss: 0.4201, valid loss: 0.6926.\n",
            "Accuracy on valid set: 0.7947443181818182\n",
            "Validation loss decreased (0.697042 --> 0.692585).  Saving model ...\n",
            "Wed Apr 28 15:10:11 2021 Epoch: 6\n",
            "Epoch 6, train loss: 0.3712, valid loss: 0.6800.\n",
            "Accuracy on valid set: 0.8075284090909091\n",
            "Validation loss decreased (0.692585 --> 0.680046).  Saving model ...\n",
            "Wed Apr 28 15:14:18 2021 Epoch: 7\n",
            "Epoch 7, train loss: 0.3598, valid loss: 0.7329.\n",
            "Accuracy on valid set: 0.7833806818181818\n",
            "1 epochs of increasing val loss\n",
            "Wed Apr 28 15:18:24 2021 Epoch: 8\n",
            "Epoch 8, train loss: 0.3388, valid loss: 0.7034.\n",
            "Accuracy on valid set: 0.8039772727272727\n",
            "2 epochs of increasing val loss\n",
            "Wed Apr 28 15:22:31 2021 Epoch: 9\n",
            "Epoch 9, train loss: 0.3220, valid loss: 0.6990.\n",
            "Accuracy on valid set: 0.8053977272727273\n",
            "3 epochs of increasing val loss\n",
            "Wed Apr 28 15:26:38 2021 Epoch: 10\n",
            "Epoch 10, train loss: 0.3000, valid loss: 0.6776.\n",
            "Accuracy on valid set: 0.8068181818181818\n",
            "Validation loss decreased (0.680046 --> 0.677633).  Saving model ...\n",
            "Wed Apr 28 15:30:45 2021 Epoch: 11\n",
            "Epoch 11, train loss: 0.2878, valid loss: 0.6849.\n",
            "Accuracy on valid set: 0.7961647727272727\n",
            "1 epochs of increasing val loss\n",
            "Wed Apr 28 15:34:52 2021 Epoch: 12\n",
            "Epoch 12, train loss: 0.2788, valid loss: 0.7086.\n",
            "Accuracy on valid set: 0.7911931818181818\n",
            "2 epochs of increasing val loss\n",
            "Wed Apr 28 15:38:59 2021 Epoch: 13\n",
            "Epoch 13, train loss: 0.2865, valid loss: 0.8003.\n",
            "Accuracy on valid set: 0.7826704545454546\n",
            "3 epochs of increasing val loss\n",
            "Wed Apr 28 15:43:05 2021 Epoch: 14\n",
            "Epoch 14, train loss: 0.2619, valid loss: 0.7207.\n",
            "Accuracy on valid set: 0.7947443181818182\n",
            "4 epochs of increasing val loss\n",
            "Wed Apr 28 15:47:11 2021 Epoch: 15\n",
            "Epoch 15, train loss: 0.2416, valid loss: 0.6900.\n",
            "Accuracy on valid set: 0.8061079545454546\n",
            "5 epochs of increasing val loss\n",
            "Wed Apr 28 15:51:18 2021 Epoch: 16\n",
            "Epoch 16, train loss: 0.2435, valid loss: 0.7003.\n",
            "Accuracy on valid set: 0.8025568181818182\n",
            "6 epochs of increasing val loss\n",
            "Stopping training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXN_1IJfPsYi",
        "outputId": "0a6f831d-f47d-4f76-ad3d-9773bac32ad6"
      },
      "source": [
        "#загрузим лучшую модель и проведем на ней инференс (прогон тестовых данных)\n",
        "model.state_dict(torch.load('/content/model.pt'))\n",
        "print(1)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQ8Z_93MPuWh",
        "outputId": "5803603a-9e2e-4263-face-5c4200766793"
      },
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data, target in testloader:\n",
        "    data = data.to(device=device)\n",
        "    target = target.to(device=device)\n",
        "    outputs = model(data)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    correct += (predicted == target).sum().item()\n",
        "\n",
        "\n",
        "acc = correct / len(test_set)\n",
        "print(f'Accuracy on test set: {acc}')"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test set: 0.8103864734299517\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEpiRyrkPv-0"
      },
      "source": [
        "#Проверим работоспособность\n",
        "samples, labels = next(iter(testloader))\n",
        "plt.figure(figsize=(16,24))\n",
        "grid_imgs = torchvision.utils.make_grid(samples[:24])\n",
        "np_grid_imgs = grid_imgs.numpy()\n",
        "#  чтобы вывести изображение, нужно его немного преобразовать из тензора обратно в нампай\n",
        "print(labels)\n",
        "plt.imshow(np.transpose(np_grid_imgs, (1,2,0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv0QfdkdP1a7"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "df_preds = pd.DataFrame()\n",
        "\n",
        "file_dir = '/content/ttest'\n",
        "\n",
        "name_files = os.listdir(file_dir)\n",
        "for i in range(0, len(name_files)):\n",
        "  name_files[i] = int(name_files[i].replace(\".jpg\", \"\"))\n",
        "name_files.sort()\n",
        "for i in range(0, len(name_files)):\n",
        "  name_files[i] = str(name_files[i]) + \".jpg\"\n",
        "\n",
        "for i in range(0, len(name_files)):\n",
        "    img = cv2.imread(file_dir + '/' + str(name_files[i]))[:, ::-1]\n",
        "    img = data_transforms_test(image=img)['image'].cuda()\n",
        "    pred = model(img[None])\n",
        "    \n",
        "    df_preds = df_preds.append(\n",
        "        {'image': str(name_files[i]), 'labels': encoder.inverse_transform(torch.argmax(pred.cpu(), dim=1))[0]},\n",
        "        ignore_index=True)\n",
        "\n",
        "df_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0U5XGWbP2_y"
      },
      "source": [
        "df_preds.to_csv('/content/submission.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_QHZAABP4J_"
      },
      "source": [
        "with zipfile.ZipFile('/content/drive/MyDrive/taskonML/ttest.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall('/content')\n",
        "\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/taskonML/model.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall('/content')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7yzIJIDP4_j"
      },
      "source": [
        "weight = '/content/model.pt'\n",
        "path_to_dataset = '/content/ttest/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv8ZU8C5P60K"
      },
      "source": [
        "def infer(weight, path_to_dataset):\n",
        "  from pathlib import Path\n",
        "  from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "  img_size = 256\n",
        "\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model = torchvision.models.resnet152(pretrained=True, progress=True)\n",
        "  model.fc = nn.Linear(2048, 22)\n",
        "  model.load_state_dict(torch.load(weight))\n",
        "\n",
        "  name_files = os.listdir(path_to_dataset)\n",
        "  for i in range(0, len(name_files)):\n",
        "   name_files[i] = int(name_files[i].replace('.jpg', ''))\n",
        "  name_files.sort()\n",
        "  for i in range(0, len(name_files)):\n",
        "   name_files[i] = str(name_files[i]) + '.jpg'\n",
        "\n",
        "  labels_class = ['badminton',\n",
        "                  'baseball',\n",
        "                  'basketball',\n",
        "                  'boxing',\n",
        "                  'chess',\n",
        "                  'cricket',\n",
        "                  'fencing',\n",
        "                  'football',\n",
        "                  'formula1',\n",
        "                  'gymnastics',\n",
        "                  'hockey',\n",
        "                  'ice_hockey',\n",
        "                  'kabaddi',\n",
        "                  'motogp',\n",
        "                  'shooting',\n",
        "                  'swimming',\n",
        "                  'table_tennis',\n",
        "                  'tennis',\n",
        "                  'volleyball',\n",
        "                  'weight_lifting',\n",
        "                  'wrestling',\n",
        "                  'wwe']\n",
        "\n",
        "  data_transforms = albumentations.Compose([\n",
        "    albumentations.Resize(img_size, img_size),\n",
        "    albumentations.Normalize(),\n",
        "    AT.ToTensor()\n",
        "    ]) \n",
        "  \n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  df_preds = pd.DataFrame()\n",
        "  for i in range(0, len(name_files)):\n",
        "    img = cv2.imread(path_to_dataset + '/' + str(name_files[i]))[:, ::-1]\n",
        "    img = data_transforms(image=img)['image'].cuda()\n",
        "    pred = model(img[None])\n",
        "    \n",
        "    df_preds = df_preds.append(\n",
        "        {'image': str(name_files[i]), 'labels': labels_class[torch.argmax(pred.cpu(), dim=1)]},\n",
        "        ignore_index=True)\n",
        "  \n",
        "  df_preds.to_csv('/content/submission.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaTUooktP7wM"
      },
      "source": [
        "infer(weight, path_to_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}